{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientTapeIntro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5YpPTwhw9wn",
        "outputId": "2813b577-9d24-4b18-dab0-6e5b793daa62"
      },
      "source": [
        "import random \r\n",
        "import numpy as np \r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "random.random()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47763991613160073"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nVmxepqw_QL",
        "outputId": "20cda0ef-621a-4dc7-8354-2f9624a29ad8"
      },
      "source": [
        "x_train = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\r\n",
        "y_train = x_train *2 - 1\r\n",
        "\r\n",
        "x_train, y_train"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.]),\n",
              " array([-3., -1.,  1.,  3.,  5.,  7.,  9., 11.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtmAe-zkxNsq"
      },
      "source": [
        "#weight and bias tensors. \r\n",
        "\r\n",
        "w = tf.Variable(random.random(), dtype=\"float32\", trainable=True)\r\n",
        "b = tf.Variable(random.random(), dtype=\"float32\", trainable=True)\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vAcGcd8xeaD"
      },
      "source": [
        "def loss_func(y_true, y_pred):\r\n",
        "  return tf.abs(y_true - y_pred) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci9oTPKFxjT3"
      },
      "source": [
        "'''Not sure yet why we're passing persistent=True to the GradientTape constructor yet. but when you don't do \r\n",
        "it below operation gives the following error :\r\n",
        "RuntimeError: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)\r\n",
        "'''\r\n",
        "\r\n",
        "def gradient_descent_step(x_train, y_train, lr):\r\n",
        "  with tf.GradientTape(persistent=True) as tape:\r\n",
        "    #feedforward\r\n",
        "    y_pred = w * x_train + b # since x_train and w both are scalars no need for matmul. \r\n",
        "    #compute loss .\r\n",
        "    loss = loss_func(y_train, y_pred)\r\n",
        "\r\n",
        "    \r\n",
        "  #compute gradients using the gradient tape object. (it works kind of like a context apparently). \r\n",
        "  # all the operations within that context are tracted by some kind of autodiff backend. \r\n",
        "  #NOTE: TF warns that it is a lot less efficient to make this call from within the tape context.\r\n",
        "  w_grad = tape.gradient(loss, w)\r\n",
        "  b_grad = tape.gradient(loss, b)\r\n",
        "\r\n",
        "  #weight update.\r\n",
        "  w.assign_sub(w_grad *lr)\r\n",
        "  b.assign_sub(b_grad *lr)  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHBSvsuAxyKr",
        "outputId": "555f41de-9b86-4507-bb24-6a1d38626807"
      },
      "source": [
        "def train_model(x_train, y_train, epochs, lr): \r\n",
        "  for epoch in range(epochs):\r\n",
        "    gradient_descent_step(x_train, y_train, lr)\r\n",
        "\r\n",
        "    #log epoch data every 10 epochs.\r\n",
        "    if epoch % 10 == 0:\r\n",
        "      print(f\"{epoch} has completed!\")\r\n",
        "\r\n",
        "train_model(x_train, y_train, 500, 0.001)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 has completed!\n",
            "10 has completed!\n",
            "20 has completed!\n",
            "30 has completed!\n",
            "40 has completed!\n",
            "50 has completed!\n",
            "60 has completed!\n",
            "70 has completed!\n",
            "80 has completed!\n",
            "90 has completed!\n",
            "100 has completed!\n",
            "110 has completed!\n",
            "120 has completed!\n",
            "130 has completed!\n",
            "140 has completed!\n",
            "150 has completed!\n",
            "160 has completed!\n",
            "170 has completed!\n",
            "180 has completed!\n",
            "190 has completed!\n",
            "200 has completed!\n",
            "210 has completed!\n",
            "220 has completed!\n",
            "230 has completed!\n",
            "240 has completed!\n",
            "250 has completed!\n",
            "260 has completed!\n",
            "270 has completed!\n",
            "280 has completed!\n",
            "290 has completed!\n",
            "300 has completed!\n",
            "310 has completed!\n",
            "320 has completed!\n",
            "330 has completed!\n",
            "340 has completed!\n",
            "350 has completed!\n",
            "360 has completed!\n",
            "370 has completed!\n",
            "380 has completed!\n",
            "390 has completed!\n",
            "400 has completed!\n",
            "410 has completed!\n",
            "420 has completed!\n",
            "430 has completed!\n",
            "440 has completed!\n",
            "450 has completed!\n",
            "460 has completed!\n",
            "470 has completed!\n",
            "480 has completed!\n",
            "490 has completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LSIIsLDzN9W",
        "outputId": "ddb0b320-c19d-4d89-c2fe-1ac2d84e2d64"
      },
      "source": [
        "#Investigate the last value to see whether we fit the line properly.  2x - 1 is the line where w:2 , b:-1\r\n",
        "\r\n",
        "w.numpy(),b.numpy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.008591, -0.9947848)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYCs7QHbzuEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}