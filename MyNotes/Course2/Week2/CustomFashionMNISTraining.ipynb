{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomFashionMNISTraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yDIxuhV1teo"
      },
      "source": [
        "import tensorflow as tf \r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras import layers\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSXzq6Mv1zhz"
      },
      "source": [
        "Define the model and initialize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0HQZ_yw1ydr"
      },
      "source": [
        "def base_model(): \r\n",
        "  input_ = layers.Input(shape=(784,), name=\"input\")\r\n",
        "  dense1 = layers.Dense(64, activation=\"relu\", name=\"dense1\")(input_)\r\n",
        "  dense2 = layers.Dense(64, activation=\"relu\", name=\"dense2\")(dense1)\r\n",
        "  output = layers.Dense(10, activation=\"softmax\", name=\"output\")(dense2)\r\n",
        "  model = tf.keras.models.Model(inputs = input_, outputs= output)\r\n",
        "  return model\r\n",
        "\r\n",
        "model = base_model()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbGINciC2ZOx"
      },
      "source": [
        "Build the data pipeline using TFDS Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je1rX4IN2XhL"
      },
      "source": [
        "train_data = tfds.load(\"fashion_mnist\", split=\"train\")\r\n",
        "test_data = tfds.load(\"fashion_mnist\", split=\"test\")\r\n",
        "\r\n",
        "def preprocess_func(data):\r\n",
        "  #each item in the Tensorflow Dataset object has an 'image' and 'label'. \r\n",
        "  image = data[\"image\"]\r\n",
        "  #flatten \r\n",
        "  image = tf.reshape(image, [-1]) #this syntax tells, I want just one dimension, you infer the size of it based on input.\r\n",
        "  #cast to float and normalize.\r\n",
        "  image = tf.cast(image, dtype=tf.float32)\r\n",
        "  image = image/255. #squeeze pixel values between 0 and 1.\r\n",
        "  return image, data[\"label\"]\r\n",
        "\r\n",
        "\r\n",
        "train = train_data.map(preprocess_func)\r\n",
        "test = test_data.map(preprocess_func)\r\n",
        "\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "train_prefetch = train.shuffle(buffer_size=1024).batch(BATCH_SIZE)\r\n",
        "test_prefetch = test.batch(BATCH_SIZE) #no need to shuffle the test set.\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKNHM3Yd7q_a"
      },
      "source": [
        "Model Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgbhvexX7Fcj"
      },
      "source": [
        "criterion = tf.keras.losses.SparseCategoricalCrossentropy() \r\n",
        "optimizer = tf.keras.optimizers.Adam() \r\n",
        "\r\n",
        "\r\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\r\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy() "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOdZA3us7wBU"
      },
      "source": [
        "def apply_gradient(optimizer, model, xs, ys, lr): \r\n",
        "  with tf.GradientTape() as tape: \r\n",
        "    logits = model(xs)\r\n",
        "    loss = criterion(ys, logits)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\r\n",
        "  \r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights)) # in this case, this is going to be an adam object.\r\n",
        "  return logits, loss\r\n",
        "\r\n",
        "\r\n",
        "def train_single_epoch():\r\n",
        "  losses = [] #accumulates losses from each batch in this epoch.\r\n",
        "  #for each batch\r\n",
        "  for step, (image_batch, label_batch) in enumerate(train_prefetch):\r\n",
        "    logits, loss_value = apply_gradient(optimizer, model, image_batch, label_batch, 0.001) #hardcoding the lr here... \r\n",
        "    losses.append(loss_value)\r\n",
        "    train_acc_metric.update_state(label_batch, logits) # accumulate the metric in each batch.\r\n",
        "\r\n",
        "  return losses \r\n",
        "\r\n",
        "def validation_single_epoch():\r\n",
        "  losses = [] \r\n",
        "  for step, (image_batch, label_batch) in enumerate(test_prefetch):\r\n",
        "    val_logits = model(image_batch)\r\n",
        "    val_loss = criterion(label_batch, val_logits)\r\n",
        "    losses.append(val_loss)\r\n",
        "    val_acc_metric.update_state(label_batch, val_logits) # accumulating the metric.\r\n",
        "\r\n",
        "  return losses\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6lZ_4URB4kO"
      },
      "source": [
        "Main training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx8RHJXrAdub",
        "outputId": "7640fcc3-27ec-4d10-93bb-f71882c9e76c"
      },
      "source": [
        "EPOCHS = 20 \r\n",
        "\r\n",
        "tr_metrics = [] \r\n",
        "val_metrics = [] \r\n",
        "\r\n",
        "epoch_tr_losses = [] \r\n",
        "epoch_val_losses = [] \r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "   epoch_loss = np.mean(train_single_epoch())\r\n",
        "   epoch_tr_losses.append(epoch_loss)\r\n",
        "   \r\n",
        "   epoch_acc = train_acc_metric.result()\r\n",
        "   tr_metrics.append(epoch_acc)\r\n",
        "   train_acc_metric.reset_states()\r\n",
        "  \r\n",
        "   epoch_val_loss = np.mean(validation_single_epoch())\r\n",
        "   epoch_val_losses.append(epoch_val_loss)\r\n",
        "\r\n",
        "   val_acc = val_acc_metric.result()\r\n",
        "   val_metrics.append(val_acc)\r\n",
        "   val_acc_metric.reset_states()\r\n",
        "\r\n",
        "   print(f\"Epoch: {epoch}, loss: {epoch_loss}, acc: {epoch_acc}, val_loss: {epoch_val_loss}, val_acc: {val_acc}\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "   \r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 0.5411069989204407, acc: 0.8104166388511658, val_loss: 0.44139793515205383, val_acc: 0.8429999947547913\n",
            "Epoch: 1, loss: 0.3934357166290283, acc: 0.8582833409309387, val_loss: 0.4103740155696869, val_acc: 0.8514999747276306\n",
            "Epoch: 2, loss: 0.3523063361644745, acc: 0.8726833462715149, val_loss: 0.4024989604949951, val_acc: 0.8585000038146973\n",
            "Epoch: 3, loss: 0.3274795711040497, acc: 0.879883348941803, val_loss: 0.36439579725265503, val_acc: 0.8709999918937683\n",
            "Epoch: 4, loss: 0.31343191862106323, acc: 0.8851666450500488, val_loss: 0.37132149934768677, val_acc: 0.868399977684021\n",
            "Epoch: 5, loss: 0.2950303554534912, acc: 0.8914666771888733, val_loss: 0.3520917296409607, val_acc: 0.8770999908447266\n",
            "Epoch: 6, loss: 0.287485271692276, acc: 0.8942000269889832, val_loss: 0.3667902946472168, val_acc: 0.8705000281333923\n",
            "Epoch: 7, loss: 0.27487289905548096, acc: 0.8976666927337646, val_loss: 0.35601145029067993, val_acc: 0.8755000233650208\n",
            "Epoch: 8, loss: 0.2655051350593567, acc: 0.9004833102226257, val_loss: 0.3427671790122986, val_acc: 0.8816999793052673\n",
            "Epoch: 9, loss: 0.25791677832603455, acc: 0.9039666652679443, val_loss: 0.3400239944458008, val_acc: 0.882099986076355\n",
            "Epoch: 10, loss: 0.25040021538734436, acc: 0.9074166417121887, val_loss: 0.3648066520690918, val_acc: 0.8740000128746033\n",
            "Epoch: 11, loss: 0.24313616752624512, acc: 0.9092833399772644, val_loss: 0.36316508054733276, val_acc: 0.8759999871253967\n",
            "Epoch: 12, loss: 0.23793108761310577, acc: 0.9106666445732117, val_loss: 0.35041627287864685, val_acc: 0.8817999958992004\n",
            "Epoch: 13, loss: 0.2297237068414688, acc: 0.9150833487510681, val_loss: 0.3558458089828491, val_acc: 0.8810999989509583\n",
            "Epoch: 14, loss: 0.224371999502182, acc: 0.9158666729927063, val_loss: 0.3635157644748688, val_acc: 0.8806999921798706\n",
            "Epoch: 15, loss: 0.2169194519519806, acc: 0.9174500107765198, val_loss: 0.37780705094337463, val_acc: 0.8787999749183655\n",
            "Epoch: 16, loss: 0.21419325470924377, acc: 0.92003333568573, val_loss: 0.35020941495895386, val_acc: 0.8853999972343445\n",
            "Epoch: 17, loss: 0.20941710472106934, acc: 0.9215166568756104, val_loss: 0.3611663579940796, val_acc: 0.8827000260353088\n",
            "Epoch: 18, loss: 0.202157124876976, acc: 0.9241333603858948, val_loss: 0.38006484508514404, val_acc: 0.8755000233650208\n",
            "Epoch: 19, loss: 0.19900484383106232, acc: 0.9248999953269958, val_loss: 0.39572885632514954, val_acc: 0.8761000037193298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxRuNzl3D2XL"
      },
      "source": [
        "t = train_single_epoch()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Cqn9z3EPIV"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}