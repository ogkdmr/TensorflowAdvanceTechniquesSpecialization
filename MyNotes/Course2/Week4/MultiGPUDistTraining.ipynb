{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiGPUDistTraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zg5K39A_CTI"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSfBiQF9_l7M"
      },
      "source": [
        "## Define the distribution strategy ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSeNjF4N_Gpi",
        "outputId": "0a08ff5e-167c-4254-f239-86388943d87e"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\r\n",
        "NUM_WORKERS = strategy.num_replicas_in_sync"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cnjq5H4_zXy"
      },
      "source": [
        "## Download and Preprocess the Data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKWLLD2a_NDi",
        "outputId": "6f587cab-b3bf-4636-f32f-75037500d72f"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\r\n",
        "x_train, x_test = x_train/np.float32(255), x_test/ np.float32(255) \r\n",
        "\r\n",
        "#add one dimension to the end of the data so that Conv2D can work with it. \r\n",
        "x_train = tf.expand_dims(x_train, axis=-1)\r\n",
        "x_test = tf.expand_dims(x_test, axis=-1)\r\n",
        "\r\n",
        "print(x_train.shape, x_test.shape)\r\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (10000, 28, 28, 1)\n",
            "(60000,) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83lpC5BY_wUj"
      },
      "source": [
        "BUFFER_SIZE = len(x_train) // 10 \r\n",
        "BATCH_SIZE_PER_WORKER = 64 \r\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_WORKER * NUM_WORKERS"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_emaIThM_ZfS"
      },
      "source": [
        "#convert into tf.data.Datasets, shuffle and batch. \r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\r\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhkTsbzDCimO"
      },
      "source": [
        "**When working with multiple GPUs, it is important to cast the dataset into a disributed dataset. This way, Tensorflow can perform data parallelism with this dataset on multiple workers.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iaZLamRAYz3"
      },
      "source": [
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QagKqvWPDBsb"
      },
      "source": [
        "## Define Simple Sequential Model (Nothing New Here) ##  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZF7QNQwC--p"
      },
      "source": [
        "#This function has to be called from within the scope of the strategy object.\r\n",
        "\r\n",
        "def build_model():\r\n",
        "  model = tf.keras.Sequential([\r\n",
        "                             tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", input_shape=(28,28,1)),\r\n",
        "                             tf.keras.layers.MaxPooling2D(2), \r\n",
        "                             tf.keras.layers.Flatten(), \r\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\"), \r\n",
        "                             tf.keras.layers.Dense(10, activation=\"softmax\")\r\n",
        "  ])\r\n",
        "  return model "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsTW60zWD2BL"
      },
      "source": [
        "## Custom Training Loop ## \r\n",
        "\r\n",
        "-instead of simply calling model.compile(...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MryP5lNaDno5"
      },
      "source": [
        "with strategy.scope():\r\n",
        "  #for sake of in-depth learning, I will manually reduce the losses (sum) across all workers myself. \r\n",
        "  #if I didn't pass this parameter to the loss object, it would have done it for me automatically.\r\n",
        "  criterion = tf.keras.losses.SparseCategoricalCrossentropy(reduction= tf.keras.losses.Reduction.NONE) \r\n",
        "\r\n",
        "  def compute_loss(y_true, y_pred):\r\n",
        "    # since explicitly told it not to reduce, it will return a loss from each example. \r\n",
        "    per_example_loss = criterion(y_true, y_pred) \r\n",
        "\r\n",
        "    #will sum up the losses from each example.\r\n",
        "    \r\n",
        "    '''\r\n",
        "    WHY ARE WE DOING THIS THOUGH? \r\n",
        "     -- BECAUSE WE NEED TO SCALE THESE PER-EXAMPLE LOSSES BY GLOBAL_BATCH_SIZE SO THAT WHEN distributed_train_step ADDS THEM UP,\r\n",
        "     THEY WILL BE SCALED BY THE GLOBAL BATCH SIZE, WHICH IS THE TOTAL NUMBER OF EXAMPLES, PER BATCH, ACROSS ALL WORKERS.\r\n",
        "\r\n",
        "    IF WE DIDN'T DO THIS:\r\n",
        "    --Then by default, this loss would be scaled by the number of examples that was sent to a given worker, but these numbers may not\r\n",
        "    be equal, so that the loss would be pulled towards the worker whose batch size is the smallest.\r\n",
        "    '''\r\n",
        "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n",
        "\r\n",
        "    #Reduce by getting the average of the losses.  \r\n",
        "    test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\r\n",
        "\r\n",
        "    #Metrics. \r\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\") \r\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")\r\n",
        "\r\n",
        "    #Optimizer\r\n",
        "    optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "    #Create the model within the scope.\r\n",
        "    model = build_model()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rjJuO0MG9nk"
      },
      "source": [
        "## Train and Test Step Functions ##\r\n",
        "\r\n",
        "-Notice that these functions are decorated with @tf.function. This way, autograd will generate graph mode code for them and they'll be ran on graph mode, potentially providing dramatic performance increase. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg3yFkzSGvb9"
      },
      "source": [
        "'''\r\n",
        "Notice that this function is doing nothing but calling the train_step function in the strategy.run()\r\n",
        "function. strategy.run() allows for distributing a workload according to a given strategy (of which 'strategy' is an object)\r\n",
        "'''\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def distributed_train_step(dataset_inputs):\r\n",
        "  #using strategy.run() function to distribute the execution of a given function according to the strategy.\r\n",
        "  per_replica_losses = strategy.run(train_step, args = (dataset_inputs, )) \r\n",
        "  #tf.print(per_replica_losses)\r\n",
        "  \r\n",
        "  #using strategy.reduce(), we will be adding up the loss from each \r\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\r\n",
        "\r\n",
        "\r\n",
        "'''\r\n",
        "Notice that there's no need to decorate this function with @tf.function.\r\n",
        "This is because it will be called in a function that is decorated (right above).\r\n",
        "Thus, train_step itself will be executed in graph mode.\r\n",
        "'''\r\n",
        "def train_step(inputs, LR=0.001): \r\n",
        "  images, labels = inputs \r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    y_preds = model(images)\r\n",
        "    batch_loss = compute_loss(labels, y_preds)\r\n",
        "    \r\n",
        "  gradients = tape.gradient(batch_loss, model.trainable_weights)\r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n",
        "  train_accuracy.update_state(labels, y_preds)\r\n",
        "  return batch_loss \r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def distributed_test_step(dataset_inputs):\r\n",
        "  per_replica_losses = strategy.run(test_step, args=(dataset_inputs,))\r\n",
        "\r\n",
        "\r\n",
        "def test_step(inputs):\r\n",
        "  images, labels = inputs \r\n",
        "\r\n",
        "  ''' make damn sure that you're setting training to False here.\r\n",
        "  otherwise, layers like Dropout and BatchNorm that should only be active during training \r\n",
        "  might be active during testing, contaminating the evaluation. ''' \r\n",
        "\r\n",
        "  y_preds = model(images, training=False) \r\n",
        "  loss = compute_loss(labels, y_preds)\r\n",
        "  test_loss.update_state(loss)\r\n",
        "  test_accuracy.update_state(labels, y_preds)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Cc36e7L5r1"
      },
      "source": [
        "## Finally, Training Loop ## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0dNAC7kJQ2k",
        "outputId": "1fedf802-6092-4942-f387-e57932a61fef"
      },
      "source": [
        "EPOCHS =10 \r\n",
        "template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  #training \r\n",
        "  epoch_loss = 0.0\r\n",
        "  step = 0 \r\n",
        "  #step = number of batches in the distributed dataset.\r\n",
        "  for batch in train_dist_dataset:\r\n",
        "    epoch_loss += distributed_train_step(batch)\r\n",
        "    step += 1\r\n",
        "  train_loss = epoch_loss / step\r\n",
        "\r\n",
        "  #testing \r\n",
        "  for batch in test_dist_dataset:\r\n",
        "    distributed_test_step(batch)\r\n",
        "\r\n",
        "  print(template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\r\n",
        "  \r\n",
        "  #reset the metrics' accumulators.\r\n",
        "  test_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.006500512361526489, Accuracy: 99.59720611572266, Test Loss: 0.49756261706352234, Test Accuracy: 91.91999816894531\n",
            "Epoch 2, Loss: 0.003369492245838046, Accuracy: 99.9566650390625, Test Loss: 0.5027914643287659, Test Accuracy: 92.11000061035156\n",
            "Epoch 3, Loss: 0.00244527799077332, Accuracy: 99.98333740234375, Test Loss: 0.5107318162918091, Test Accuracy: 92.20999908447266\n",
            "Epoch 4, Loss: 0.0019254875369369984, Accuracy: 99.99500274658203, Test Loss: 0.5217400789260864, Test Accuracy: 92.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-x5gCfM8-X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}